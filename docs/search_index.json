[
["index.html", "XAI Stories Preface", " XAI Stories 2020-05-08 Preface This book is the result of a student projects for Interpretable Machine Learning course at the University of Warsaw and the Warsaw University of Technology. Each team has prepared one case study for selected XAI technique. This project is inspired by a fantastic book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich. We used the LIML project as the cornerstone for this repository. The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using github repository. Cover by kozaka93. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. References "],
["foreword.html", "Foreword 0.1 Why? 0.2 What? 0.3 How?", " Foreword Author: Przemyslaw Biecek (Warsaw University of Technology and University of Warsaw) 0.1 Why? Machine learning has a number of applications. Very often, however, machine learning predictive models are treated as black boxes which can be automatically trained without worrying about the domain in which they are used. This opaqueness rises many risks that are difficult to foresee during the model building process. Such as the model’s declining performance due to the data drift, poor performance on the out-of-domain problems or unfair biased behaviour learned on historical data. The growing list of examples where black boxes fail spectacularly has led to an increased interest in XAI methods. Such methods allow to x-ray black boxes models for more detailed analysis on the local or global level. According to Gartner Hype Cycle for Emerging Technologies in 2019 Explainable AI is on the verge of Innovation trigger and Peak of inflated expectations. It is a technology with a very high potential, which is talked about a lot in the media and which heats up the imagination as strongly as AI. In the literature there are many articles arguing the need to use XAI methods as well as many ideas for new methods from the XAI family. However, it is much more difficult to find examples of successful implementations of XAI methods that have improved the business. Missing elements are case studies of actual use of XAI methods in machine learning problems. Such case studies would allow a better understanding of what is possible today and what is not possible using XAI methods. 0.2 What? This ebook collects examples of the use of different methods from the XAI family for different real-world predictive problems. In the following chapters, we show example applications of different XAI techniques to problems based on real-world public dataset. These examples are called XAI stories and like every good story, each one has a structure. It starts with a description of the predictive problem, goes on to describe the proposed model or models. The models are x-rays using XAI techniques to finish the chapter with a point. 0.3 How? For XAI stories to be credible they need not only a strong predictive model, but also business validation of the proposed modeling and explanation approach. Each group of students got two mentors from McKinsey’s Data Science department. The mentors, together with the students, searched for strengths and weaknesses of XAI applications in specific problems. TODO: write more about this collaboration "],
["story-house-sale-prices.html", "Chapter 1 Story House Sale Prices: eXplainable predictions for house sale 1.1 Introduction 1.2 Model 1.3 Explanations 1.4 Summary and conclusions", " Chapter 1 Story House Sale Prices: eXplainable predictions for house sale Authors: Piotr Grązka (SGH Warsaw School of Economics), Anna Kozak (Warsaw University of Technology), Paweł Wicherek (Warsaw University of Technology) Mentors: Mateusz Zawisza (McKinsey &amp; Company), Adam Zmaczyński (McKinsey &amp; Company) ,,That’s all your house is: it’s a place to keep your stuff while you go out and get more stuff.’’ George Carlin 1.1 Introduction Everybody needs a roof over their heads. It can be a house, villa or a flat. Everybody, at some point of life, faces a choice if to buy a house. If so, which one. And why they are so expensive? Topic of real estates is not only the topic you just have to deal with. It can also be very interesting. There are plenty of TV Shows, for instance Property Brothers, of which plot is based on examples of people buying and renovating houses. This particular one is the most famous in the world and has been running already almost a decade. For many people houses are also products to buy and sell with income. Regardless of motives of buy/sell real estate, both sides agree to a price. It is always good to know, how much it is worth, what’s the fair/true value. And, maybe it’s even more important, why the price is like that, what has an influence on it. In this work we want to try to find an answer to both questions with stronger emphasis on the second one. This paper is intended to be a complete use cases how to deal with regression problem for Data Scientists. Let’s start with a couple of questions that will allow us to understand and define the problems The seller does not know how to increase the cost of the apartment so that the investment outlay is lower than the added value (e.g. building a pool will increase the price and renovating the bathroom is not worth it). The seller does not know how much to sell the apartment for (he makes an offer on the portal and does not know if the price is fair). The buyer does not know how much the apartment is worth (as above, whether the price is fair). Commercial problem: Auction services do not have tools to support sellers/buyers. These are just some of the questions we can ask. As a definition of our problem, we have set ourselves a property valuation, and through explanations we will try to get an answer depending on the position we choose. We have divided our work into several stages, below we present a diagram with a step plan. It allowed us to plan our work, and now we will use it to tell you what we did. We started our work with a literature review. Many works show a comparison of hedonistic models (linear regression) and machine learning models. Below there is a plot with comparison of results achieved by models interpretable by design (hedonistic) and black box model (ANN) based on the article @ref(house_prices). We can conclude that we reduce the interpretability to an increase in the quality of model fitting. The next point was data analysis, we work on dataset which contains house sale prices for King County, which includes Seattle. It is include homes sold between May 2014 and May 2015. Data available on kaggle and openml. We have analyzed the data. More link. Data contains 19 house features plus the price and the id columns, along with 21613 observations. id unique ID for each home sold date date of the home sale price price of each home sold bedrooms number of bedrooms bathrooms number of bathrooms, where .5 accounts for a room with a toilet but no shower sqft_living square footage of the apartments interior living space sqft_lot square footage of the land space floors sumber of floors waterfront apartment was overlooking the waterfront or not view how good the view of the property was condition condition of the apartment grade level of construction and design sqft_above the square footage of the interior housing space that is above ground level sqft_basement the square footage of the interior housing space that is below ground level yr_built the year the house was initially built yr_renovated the year of the house’s last renovation zipcode zipcode area lat lattitude long longitude sqft_living15 the square footage of interior housing living space for the nearest 15 neighbors sqft_lot15 the square footage of the land lots of the nearest 15 neighbors We have collected methods to evaluate the performance of the regression model, we decided to use RMSRE (root mean square relative errors), because the difference of 50 thousand on a property worth millions and 50 thousand on a property worth 200 thousand is quite considerable. More about measures link. Based on the literature we decided to test the following models: - fixed effects model - random effects model - Spatial Autoregressive (SAR) model - linear regression - decision tree - random forest - gradient boosting, xgboost Another idea how to enrich our solution was to add external data. The location of the property can significantly affect the price, so we also took into account the distance from public transport and the number of cultural facilities within a kilometre radius. 1.2 Model Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. 1.3 Explanations Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 1.4 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? "],
["story-hotel-booking.html", "Chapter 2 Story hotel booking: eXplainable predictions of booking cancelation and guests coming back 2.1 Introduction 2.2 Model 2.3 Explanations 2.4 Summary and conclusions", " Chapter 2 Story hotel booking: eXplainable predictions of booking cancelation and guests coming back Authors: Domitrz Witalis (MIM), Seweryn Karolina (MiNI) Mentors: Jakub Tyrek (Data Scientist), Aleksander Pernach (Consultant) 2.1 Introduction The dataset is downloaded from the Kaggle competition website https://www.kaggle.com/jessemostipak/hotel-booking-demand. This dataset contains booking information for a city hotel and a resort hotel in Portugal, and includes information such as when the booking was made, length of stay, the number of adults, children, babies, the number of available parking spaces, chosen meals, price etc. There are 32 features and 119 390 observations. The booking website has information about these reservation characteristics and building models can help this company in better offer management. The most important information could be the prediction of booking cancelation, the prediction if client comes back to the hotel, the prediction whether client orders additional services (eg. meals), customer segmentation. In this project, we have decided to focus on two first issues. 2.1.1 Hyperparameter optimization 2.1.2 Imbalanced dataset [Put a description of the problem here. indicate the data source. Describe why this problem is important. Indicate the most important literature on the problem.] 2.2 Model 2.2.1 Model 1. Booking cancelation The aim of this model is to predict whether guest cancels reservation and explanation of the reasons. The chosen model is XGBoost with RFE (Recursive Feature Elimination). Bayesian optimisation with TPE tuner has been applied in order to improve model performance. Neural Network Intelligence (NNI) package has been chosen for this task, because it provides user-friendly GUI with summary of experiments. List of optimized hyperparameters and search space: max_depth - the maximum depth of tree. n_esimators - the number of trees. learning_rate - boosting learning rate colsample_bytree - subsample ratio of columns when constructing each tree. Figure details paths of hyperparameters values chosen by algorithm. On the right you can see metric (AUC) of model with those parameters. Figure presents AUC for each experiment. It shows a clear trend in model performence so algorithm is choosing better and better hyperparameters. AUC train AUC test image 2.2.2 Model 2. Repeated guests Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. 2.3 Explanations 2.3.1 Model 1. Booking cancelation 2.3.1.1 dataset image . Figure above shows SHAP values. There are some interesting findings which are intuitive: Clients who canceled some reservations in the past are more likely to cancel another reservation. People who buy refundable option cancel reservations more often than others. A lot of days between reservation time and arrival time increases probability of cancelling booking. People who travel with children are more likely to cancel booking. There are also less intuitive findings: People without any special requests cancel reservetion more often than others. If trip starts at the end of the week there is higher probability that customers change their minds. The bigger number of adults, the highest probability of cancelation. 2.3.1.2 instance The lowest prediction of cancelation probability The highest prediction of cancelation probability 2.3.2 Model 2. Repeated guests Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 2.4 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? "],
["story-uplift-modelling.html", "Chapter 3 Story Uplift Modelling: eXplaining colon cancer survival rate after treatment 3.1 Introduction 3.2 Model 3.3 Explanations 3.4 Summary and conclusions", " Chapter 3 Story Uplift Modelling: eXplaining colon cancer survival rate after treatment Authors: Aleksandra Łuczak (Warsaw University of Technology), Tymoteusz Makowski (Warsaw University of Technology), Kateryna Shulikova (Warsaw School of Economics) Mentors: Miłosz Dobersztyn (McKinsey), Armin Reinert (McKinsey) 3.1 Introduction TODO(oryginał): Put a description of the problem here. indicate the data source. Describe why this problem is important. Indicate the most important literature on the problem. 3.1.1 What is Uplift Modelling? Czym jest uplift modelling? Gdzie go się stosuje? Jakie problemy możemy modelować (survival &amp; leki, marketing &amp; zysk)? 3.1.2 Dataset Description These are data from one of the first successful trials of adjuvant chemotherapy for colon cancer. Levamisole is a low-toxicity compound previously used to treat worm infestations in animals; 5-FU is a moderately toxic (as these things go) chemotherapy agent. There are two records per person, one for recurrence and one for death. Jest tutaj mowa o raku jelita grubego i eksperymentalnym podejściu do leczenia. Użyty był lek Levamisole - który jest stosowany u zwierząt jako lek przeciwrobaczy, ale zauważono jego właśniwości do leczenia raka jelita grubego. Do porównania jest drugi lek 5-FU, czyli 5-Fluoro-Uracyl, który się stosuje “standardowo” w leczeniu raka jelita grubego To jest ten “mocniejszy” lek. Oba te leki podaje się po opreacji wycięcia raka, jest to adjutowantowa chemia, czyli ‘dodatkowa pooperacyjna’. W zabiorze mamy zmienne: Variable name Description id id study 1 for all patients rx Treatment - Obs(ervation), Lev(amisole), Lev(amisole)+5-FU sex 1=male age in years obstruct obstruction of colon by tumour - jest to zwężenie okrężnicy przez tak, czyli zablokowanie przez guz perfor perforation of colon - czy zrobiła się dziura w jelicie adhere adherence to nearby organs - przyleganie do okolicznych narzadów(np pęcherza moczowego) nodes number of lymph nodes with detectable cancer - podczas operacji wycina się węzły chłonne które były zaatakowane przez raka i to jest liczba węzłów chłonych z wykrywalnym rakiem, powinno być dużo minimum 12, żeby operacja była ‘udana’ time days until event or censoring - liczba dni do rozpoczęcia leczenia? status censoring status differ differentiation of tumour (1=well, 2=moderate, 3=poor) - zróżnicowanie komórek guza - im więcej tym lepiej, bo bardziej przypomina komórki jelita, a nie zlepek nie wiadomo czego extent Extent of local spread (1=submucosa, 2=muscle, 3=serosa, 4=contiguous structures) - Zasięg nowotworu, tzn do jakich tkanek dotarł, 1=podśluzowa, 2=mięśniowa, 3=surowicza, 4=przyległe struktury (im mniej tym lepiej) surg time from surgery to registration (0=short, 1=long) - czas od operacji do rejestracji node4 more than 4 positive lymph nodes - 4 lub więcej węzłów chłonnych etype event type: 1=recurrence,2=death - nawrót lub śmierć Co na nim będziemy modelować (treatment a survival rate). Modele przewidują śmiertelność danego pacjenta w zależności odpodanego leku oraz skuteczność leku, czyli czy poprawił stan zdrowotny pacjenta, czy nie. Wstępne uzasadnienie dlaczego warto tym się zająć i dlaczego dobrze będzie tu cokolwiek wyjaśniać [ad 5]. 3.2 Model TODO(oryginał): Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. - Jaki model wybraliśmy? - Jak trzeba było przygotować dataset? - Jakie ma osiągi? - Jakieś miary typu F1, accuracy i opis tego, które miary na tym zbiorze powinno się stosować. Model do przewidywania skuteczności leku został stworzony za pomocą algorytmu ExtraTrees z pakietu sklearn. Test -&gt; AUC = 90.98% Train -&gt; AUC = 90.31% columns |importance status |0.747059 node4 |0.101439 treatment |0.026713 extent |0.024557 adhere |0.022921 nodes |0.022467 age |0.018148 perfor |0.010347 obstruct |0.007869 sex |0.007074 surg |0.006733 differ |0.004674 Model do przewidywania śmiertelności w zależności od podanego leku został stworzony za pomocą algorytmu UpliftTreeClassifier z pakietu casualml. Przewidywanie leczenia: Acc = 43.27% Przewidywanie nawrotu choroby: AUC = 55.35% 3.3 Explanations TODO:(oryginał): &gt; Here, show how XAI techniques can be used to solve the problem. &gt; Will dataset specific or instance specific techniques help more? &gt; &gt; Will XAI be useful before (pre), during (in) or after (post) modeling? &gt; &gt; What is interesting to learn from the XAI analysis? Jak w ogóle to wyjaśniać skoro np. możemy mieć dwa modele? Wizualizacje dopracować (czytelne wykresy, opisy itp.) [ad 4]. 3.3.1 Dataset Level Explainations Jaka jest istotność zmiennych? 3.3.2 Instance Level Explainations Jakieś “ciekawe” przypadki? 3.4 Summary and conclusions TODO:(oryginał): &gt; Here add the most important conclusions related to the XAI analysis. &gt; What did you learn? &gt; Where were the biggest difficulties? &gt; What else did you recommend? Jakich informacji XAI nam dostarczyło na temat tego problemu? Jak XAI sprawdza się do wyjaśniania upliftu? Dlaczego było warto to modelować (konkretnie ten dataset) [ad 5]. "],
["story-meps-regression.html", "Chapter 4 Story Meps: regression 4.1 Introduction 4.2 Model 4.3 Explanations 4.4 Summary and conclusions", " Chapter 4 Story Meps: regression Authors: Anna Kozioł (Warsaw University of Technology), Katarzyna Lorenc (Warsaw University of Technology), Piotr Podolski (University of Warsaw) Mentors: Maciej Andrzejak (Affiliation 2), Alicja Jośko (Affiliation 1) 4.1 Introduction The aim of the project is to predict medical expenses based on demographic and socio-economic variables. The Medical Expenditure Panel Survey provides data on health care as well as related costs. Data for the MEPS project has been collected regularly since 1996. Each year, approximately 15,000 households are selected as a new panel of surveyed units. 4.2 Model We are testing models and trying to improve their quality. Right now, the best model we developed is Gradient Boosting Regressor. The figures below present the prediction values on training and test set. All key information about the final model we will put in this section. 4.3 Explanations 4.4 Summary and conclusions "],
["story-meps-healthcare-expenditures-of-individuals.html", "Chapter 5 Story MEPS: Healthcare expenditures of individuals 5.1 TODOS 5.2 Introduction 5.3 Models 5.4 Explanations 5.5 Summary and conclusions", " Chapter 5 Story MEPS: Healthcare expenditures of individuals Authors: Dominika Bankiewicz (University of Warsaw), Jakub Białek (Warsaw University of Technology), Agata Pładyn (Warsaw University of Technology) Mentors: 5.1 TODOS Opisy wszystkich zmiennych do appendixa (tabela?) Rozkład/percentyle Y, przed i po transformacji (wtedy łatwiej ocenić wyniki absolutne) do wstępu. Podrozdzział z wynikami każdego z modeli? W rozdziale modele - krótki wstęp, tam napisać, o transformacji y i o tym, że korzystaliśmy z tego samego train/test splitu. 5.2 Introduction In this part, models that predict annual healthcare expenditure of individuals will be developed and analyzed using XAI methods. The data set analyzed is called MEPS (Medical Expenditure Panel Survey) and it is freely accessible at [ref1]. The data comes from large-scale surveys of families, individuals, medical providers and employers from the United States of America. Each observation of the data set contains total expenditures of an individual as well as number of other variables describing his or her demographic and socio-economic status. This allows to create models predicting the expenditure based on other factors. For this reason it is particularly interesting from the point of view of the subject that is financially responsible for the healthcare cost – insurance company, government, healthcare provider or individuals. It is important to mention, that for all of these subjects, accurate model is only a part of the success – the other part are the relations between input parameters and predictions of the model. Having that we can provide an answer not only for how much? but also for why? Hopefully, this can be achieved with available XAI methods. The data analyzed in the following sections was not downloaded directly from MEPS website. Instead, it was obtained through IBM’s AIX360 [ref2]. Therefore it is initially preprocessed – race is restricted to black and white and ethnicity to non-Hispanic, records with special values (negative integers) are removed for some of the variables, variables are initially selected. The dataset provides 18350 observations and it contains variables that describe: demographics (age, gender, marital status), socio-economics (education, income, insurance coverage), self-reported health status (self-evaluation of physical and mental health status), diagnoses (stroke, cancer, heart disease, diabetes), health limitations (cognitive, activity, sensory). The following section describes the development of three different models for predicting the transformed total health expenditure of an individual. Once developed, these models are compared in terms of their quality and of them is selected and analyzed using XAI methods. Explanations provided by different methods are discussed. The baseline for this discussion was provided by the authors of IBM’s AIX360 [ref3]. 5.3 Models In the following subchapters the development of three different models is briefly described. Full details on the implementation together with the code itself can be found here. First of all, since the distribution of the predicted variable is strongly skewed, it was transformed with logarithm base 3. Typically in such cases, natural logarithm is chosen, but having in mind that impact of input variables on the prediction will be analyzed, the decision was made to use base 3 instead (when we see that some input variable affected the prediction increasing it by one, then we can say that it increased the total expenditures by the factor of three, not factor of Euler number). In order to ensure that results from all three models can be directly compared, models are trained and evaluated on the same, arbitrarily chosen, test and validation subsets. These can also be found at [repo]. The evaluation metrics are RMSE, MAE and R^2 [ref to the table with results]. 5.3.1 Model 1: Linear Linear model was created with sklearn package. Following operations were performed to prepare the data: logarithm change of the explainable variable (as described before) min and max scaling of variables RTHLTH31, MNHLTH31, POVCAT15. This variables describe the state of general health, mental health and poverty status in values (decoded to: poor, fair, good, very good, excellent). addition of a new column which counts for how many diseases/health issues patient was tested positive. The ridge regression was performed with GridSearchCV. This allowed to perform 5-fold cross-validation with a range of different regularization parameters in order to find the optimal value of alpha parameter. The final results of the ridge regression model are shown in a table (values rounded to 2 decimal points): RMSE MAE R^2 Train 2.27 1.72 0.33 Test 2.25 1.7 0.3 Figure presents the prediction values compare to real values of target variable on training dataset. Figure presents the prediction values compare to real values of target variable on test dataset. 5.3.2 Model 2: ANN The second model evaluated was a multilayer perceptron. The input data was preprocessed with use of scikit-learn [ref to sklearn] tools: numerical features were standardized categorical features were one-hot encoded The data was fed into ANN with 4 hidden, fully-connected layers. The ANN model itself was created with Keras [ref to Keras]. Alltogether, the preprocessing steps and the model, was wrapped into scikit-learn pipeline so it can be easily use with XAI methods. The results are shown in the table below: RMSE MAE R^2 Train 2.04 1.51 0.45 Test 2.17 1.61 0.37 5.3.3 Model 3: XGB XGB model was developed using scikit-learn package (scikit-learn 2019a). For this model, data was prepared in the following way: as was mentioned in Introduction, target variable was transformed with logarithm base 3, categorical features (i.e. features with 10 or less unique values, except variables “POVCAT15”, “RTHLTH31”, “MNHLTH31” which can be treated as continuous) were transformed with OneHotEncoder (scikit-learn 2019c), numerical features were transformed with StandardScaler (scikit-learn 2019d). Hiperparameters tuning was done with GridSearchCV (scikit-learn 2019b). Following parameters were optimized: n_esimators - the number of trees, max_depth - the maximum depth of tree, min_samples_split - the minimum number of samples required to split an internal node, min_samples_leaf - the minimum number of samples required to be at a leaf node. Results for best hyperparameters shows below table. RMSE MAE R^2 Train 2.04 1.51 0.45 Test 2.17 1.61 0.37 Figure presents the prediction values compare to real values of target variable on training dataset. Figure presents the prediction values compare to real values of target variable on test dataset. Finally, for XAI methods analysis, model was wrapped in pipeline. 5.3.4 Results Table below presents results for all developed models. RMSE (train) MAE (train) R^2 (train) RMSE (test) MAE (test) R^2 (test) LR 2.27 1.72 0.33 2.25 1.70 0.30 ANN 2.14 1.59 0.40 2.17 1.61 0.37 XGB 2.04 1.51 0.45 2.17 1.61 0.37 5.4 Explanations 5.4.1 Instance level 5.4.1.1 Observation with the best prediction 5.4.1.2 Observation with the worst prediction This patient it’s a 76 years old woman. She is a widow and has a GED or high school degree. She complains about poor health status and fair mental health status. She has ever been diagnosed with high blood pressure, coronary heart disease, emphysema, chronic bronchitis, high cholesterol, cancer, rheumatoid arthritis and joint pain last 12 months. She has social and cognitive limitation and limitation in physical functioning. She doesn’t smoke and has serious difficulty see or wears glasses. Her health insurance coverage indicator is public only. Total health expenditure of this woman is equal to zero but model predicted that it is equal to about 12005.68 (3 to the power of 8.55). Figure presents Break Down plot for observation selected in this subchapter Picture below shows Shapley Values for this observation. As we can see, the largest impact on total health expenditure of this woman is the fact that she has been diagnosed with cancer, high blood pressure and how old she is. Figure presents Shapley Values for observation selected in this subchapter Three plots below show the Ceteris Paribus profiles for this observation. They say how total health expenditure of this woman would change if a variable had a different value with the other variables unchanged. They complete the chart with Shapley Values. As you can see, the fact that this woman was diagnosed with cancer increases her spending by about 3 to the power of 0.5 times, while the fact that she was diagnosed with high blood pressure increases her expenditure by about 3 to the power of 0.3 times. If she were younger by at least about 10 years, expenses would be smaller, and their would be the smallest if she were about 35-50 years old. image image image 5.4.1.3 Observation with high prediction The patient with high expected health expenses value is a 66 years old women. She is married and have a bachelor degree. Her health expenses are equal to 36 295 (9.557 to the power of 3) and the models prediction is equal to 34 658 (3 to the power of 9.515). The patient suffers from many conditions including cancer, high blood pressure, angina, heart disease (had heart attack), high cholesterol, diabetes, joint pains. She has social and physical functioning limitations. She has a private insurance. Break Down explanations of model prediction for the patient. Break Down explanations indicates that the patient’s diseases have significant positive impact on the prediction. Shap explanations of model prediction for the patient. Ceteris Paribus explanations of model prediction for the patient. When investigated changes in the patients descriptions variables there are some additional interesting results. Despite patient positive diagnosis for many diseases, the health expenses would be lower if the patient age (Ceteris Paribus plot of the variable AGE31X) would be much more younger (20 years old or less). Patients walking limitations (Ceteris Paribus plot of the variable WLKLIM31) increases the prediction up to 1.73 (3 to the power of 0.5) times. Her low result in physical component summary (Ceteris Paribus plot for the variable PCS42) also increases the expenses cost (by up to 3 to the power of 1.5 times). Ceteris Paribus plot of the variable INSCOV15 shows the relation between type of insurance and the prediction. The patient has the private insurance (value 1) and if she had changed to public or opted for none she would have lower prediction. 5.4.1.4 Observation with low prediction The patient is a 21 years old man. He has never been married. He has only graduated from high school. He is poor. He has not have any insurance. He perceives his health as at excellent state. He smokes but he has not been diagnosed with any diseases. His health expenses are equal to 0.0. Model’s prediction is equal to 3 to the power of 0.004. Break Down explanations of model prediction for the patient. The Break Down explanations indicates his overall ratings of feeling may increase the prediction but after considering all the other factors the model prediction is equal almost to 0. Shap explanations of model prediction for the patient. Shap explanations shows that his not student status lowers the prediction by 3 to the power of 0.26 times. His good eyesight, no walking limitations and lack of heart diseases lowers the prediction 3 time to the power of around 0.2 times. Generally his lack of diseases lowers the predictions. Ceteris Paribus explanations of model prediction for the patient. If the patient had been diagnosed with diabetes his health expenses would be 3 to the power of 2.5 times higher (plot of the variable DIABDX). Positive diagnoses for high cholesterol would also increase by 3 times the health expenses prediction. What is more, he is not a student, what results in lower prediction by up to 3 to the power 1.3 times (plot of the variable FTSTU31X). His poverty status decreases the prediction by almost 3 times. 5.4.2 Dataset level In this part the focus is put on model-level explanations. Variables that are interesting, intuitive or simply important from the model perspective will be investigated. 5.4.2.1 Variable importance Figure presents permutational variable importance of ten most important features for XGB model. 5.4.2.2 Age variable Intuitively, the age of the patient should be a very good predictor. Even non-experts can tell that usually the older the person is, the more issues with health it has and thus - the more money will be spend on healthcare. Lets have a look at PDP and ALE profiles of this variable in our model: ALE and PDP profiles of age variable Both profiles follow similar trends and in general they show the expected behaviour. Some people are born with health issues already, so they generate high costs at the very beginning of their lives. Then, the ones who manage to overcome these initial issues are generally healthy. Usually teenagers are in a good shape - a lot of physical activity, careful parents, regular eating at schools etc. Once they become adults (16-18) their life becomes riskier (driving license, alcohol). Some of them also start to earn their own money. All of this sums up to sharp increase in medical expenditures of this group. Then there’s another plateau and another sharp increase at the age of about 50 years. This the age when a lot of disease are getting more probable thus people are taking clinical tests, diagnose and start to cure. This is also the age when menopause happens to women which usually worsens their health status. From now on, health status becomes worse and health expenditures are usually getting higher every year. From the model developer perspective - this relation seems to be a little raged. One would expect to be more smooth and monotonic. For example, there is no particular reason why medical expenditures should decrease at once we turn 60, but the plot shows otherwise. This suggests that it might be a good idea to discretize the age variable into number of groups. While there is very small probability that your health status will get worse next year, it is highly likely that it will get worse 10 years from now. It is worth investigation whether this transformation of the variable will improve the results. From the perspective of the medical provider and patient itself - we can clearly see that there are some points at which the expenditures sharply increase. It might be good idea for a patient to buy additional insurance once he or she approach that age. On the other hand, insurance provider should take that into account while preparing his offer. If the agreement time is couple of years and it includes that specific time when health sharply worsens, this should be included in price. 5.4.2.3 Evaluation of health status Evaluation of one’s health status seems to be an obvious indicator of what can we expect in terms of health expenditure. One should be cautious though - the methodology of this measurement is crucial here. Let’s have a look at profiles of variables PCS42 and RTHLTH31 - both are self-evaluation of ones health. PCS42 was created based on set of concrete questions about specific pain, limitations in activities etc. The higher the value, the better the health. On the other hand, RTHLTH31 just describes the overall health condition in one word, corresponding to the scale from 1 to 5 (“excellent”, “very good” “good” “fair” and “poor”). In this case, the higher the value, the worse the health. Both plots show the expected behaviour of the model. It is interesting to see though, that there is not much difference between “excellent” and “very good”. Similarly - the difference is very small between “fair” and “poor”. This is primarily caused by the subjective matter of these answers. First of all the understanding of the words their selves - something which is very good for one person, might be just good for other. Second thing is that we usually compare our current health to the recent past. If one feels tired every day, he might say that his health is fair. But if he feels a little tired every day but he just recovered from flu, he will say he feels very good, or excellent (in comparison to how he felt a week ago). Finally, people tend to get used to their diseases. It is proven, that people feel very bad at the beginning - when they are diagnosed, but when the time goes by they care less and less - they just get used to living with a disease. Nevertheless, it is important to say that insurance provideres should be very careful while pricing their services partially basing on surveys. It is crucial to ask specific questions that cannot be biased by subjective feelings of the respondent. 5.5 Summary and conclusions References "],
["story-uplift-marketing1.html", "Chapter 6 Story Uplift modeling on marketing dataset: eXplainable predictions for optimized marketing campaigns 6.1 Introduction 6.2 Dataset 6.3 Model [wymaga dopracowania] 6.4 Explanations [wymaga dopracowania] 6.5 Summary and conclusions", " Chapter 6 Story Uplift modeling on marketing dataset: eXplainable predictions for optimized marketing campaigns Authors: Jan Ludziejewski (Warsaw University), Paulina Tomaszewska (Warsaw University of Technology), Andżelika Zalewska (Warsaw University of Technology) Mentors: Łukasz Frydrych (McKinsey), Łukasz Pająk (McKinsey) 6.1 Introduction Running bussiness is a challenge. It involves making a lot of decisions in order to maximize profits and cut down costs - finding the tradeoff is not a straight-forward task. Here comes Machine Learning and uplift models that can help in optimizing marketing costs. People often ask whether it makes sense to address marketing campaigns to all company’s customers. From one point of view by sending an offer we think the probability that the customer will buy our product is higher - in fact it is not always the cases (the matter will be described in details later). On the other hand, making large-scale campaign is costly. Therefore it would be good to know what is the return of investment. Above we presented the common sense arguments but how science addresses a question: “Is it true that by sending the marketing offer we only extend the chance for the customer to buy our product and therefore extend our profit?”. The issue was already investigated (Verbeke and Bravo 2017) and it was pointed out that customers of any company can be divided into 4 groups 6.1. FIGURE 6.1: Customer types taking into consideration their response to treatment The image matrix was created based on the customer decision to buy a product depending on the fact that they were addressed by marketing campaign or not. The action used for trigggering in customer the particular behaviour is called treatment. In the 4 groups we distinghuish 6.1 : the customers that irrespective of the fact that they experienced treatment or now that are going to buy a product (these are called “sure things”) the customers that irrespective of the fact that they experienced treatment or now that are NOT going to buy a product (“lost causes”) the customers that without being exposed to marketing campaing would NOT buy a product (“persuadables”) the customers that without being exposed to marketing campaing would buy a product but in case thay receive a marketing offer they resign (“sleeping dogs”) It can be than observed that in case of “lost causes” and “sure things” sending a marketing offer makes no impact therefore it doesn’t make sense to spend money on targeting these customers. As the company we should however pay more attention to the groups “persuadables” and “sleeping dogs”. In case of the first bearing the costs of marketing campaign will bring benefit. In case of the latter we not only spend money on targeting them but as the result we will also discourage them from buying the product therefore we as a company loose two times. The case of sleeping dogs can seem irrealistic, therefore we present an example. Let’s imagine there is a customer that subscriped our paid newsletter. He forgot that he pays each month fixed fee. He would continue paying unless a company sends him a discount offer. At this moment the customer realises that he doesn;t need and offer and unsubscripes. By understading the structure of the company’s customers, it can target its offer more effectively. 6.1.1 Approaches towards uplift modeling In (Akshay Kumar 2018) it was pointed out that the problem of deciding whether it is profitable to send an offer to particular customer can be tackled from two different perspectives: predictive response modelling (it is classical classification task where model assigns probability to each of the classes) and uplift modelling (where the “incremental” probability of purchase is modelled). The latter is more interesting but at the same time more challenging. Uplift modeling is a technique that helps to determine probability gain that the customer by getting the marketing materials will buy a product. The field is relatievely new. The two most commmon approaches are (Lee 2018): Two Model In this method there are build two classifiers. The one is trained on observations that received treatment (model_T1) and the second is trained on observations that didn’t receive a treatment (model_T0). Later the uplift for particular observations is calculated. If the observation experienced treatment then it is an input to the model_T1 and the probability that the customer will buy a product is calculated. Later the if condition is investigated meaning what would happen if the customer didn’t receive a treatment. In Ssuch case the treatment indicator in observation’s feature is changed to “zero”. Such modified record is an input to model_T0 that predicts the probability that such customer will buy a product. The uplift is calculated as difference between output of the model_T1 and model_T0. The higher the difference, the more profitable is addressing marketing campaign to particular customer. One Model The one model approach is similar conceptually to the Two model approach with such a difference that instead of building two classifiers only one is used. Therefore every observation is an input to the model that generates prediction. Later the indicator in the treatment column is changed into the negation and such vector is used as input to the model that once again output probability that the customer buy a product. The uplift is the difference of the two predicted probabilities. [przydałoby się stworzyć jakąś prostą grafikę tutaj] As the uplift modeling is an emenrging field there isn’t a list of good practices in terms of what classifier is better to use. In (Zaniewicz and Jaroszewicz 2013), the autors investigated application of SVM. But due to the fact that SVM requires precise, long lasting finetuning we decided to use xgboost (the architecture of our solution is described in details in section Model). 6.2 Dataset There is a scarcity of well-documented datasets dedicated to uplift modeling. Therefore the autors of (Rzepakowski and Jaroszewicz 2012) proposed to artificially modify available datasets in order to extract information about treatment. As the purpose of this story is to investigate XAI techniques in the domain of uplift modeling we decided to use real life dataset. We chose Kevin Hillstrom’s dataset from E-Mail Analytics And Data Mining Challenge (Hillstrom 2008).The dataset consists of 64000 records reflecting customers that last purchased within 12 months. As a treatment an e-mail campaign was addressed: 1/3 of customers were randomly chosen to receive an e-mail campaign featuring Mens merchandise 1/3 were randomly chosen to receive an e-mail campaign featuring Womens merchandise 1/3 were randomly chosen to not receive an e-mail campaign (“control group”) As an expected behaviour the following actions were determined: * visit the company’s website within 2 weeks after sending to the customers a marketing campaign * purchase a product from the website within 2 weeks after sending to the customers a marketing campaign In the challenge the task was to determine whether the Mens or Womens e-mail campaign was successful. In our task we reformulated the task and we want to answer the question whether any e-mail campaign was profitable for the company. The features about customers in the dataset are specified in fig 6.2: FIGURE 6.2: Customer features in the dataset There is also information about customer activity in the two weeks following delivery of the e-mail campaign (these can be interpreted as labels): Visit: 1/0 indicator, 1 = Customer visited website in the following two weeks. Conversion: 1/0 indicator, 1 = Customer purchased merchandise in the following two weeks. Spend: Actual dollars spent in the following two weeks. [zwiększę potem czcionkę na grafice] 6.2.1 Feature engineering It is largely imbalanced - there is only about 15% of positive cases in column Visit and x% in column Conversion. In such situation we decided to use column Visit as a label. As the number of column is small we decided to use one-hot encoding for transforming categorical variables instead of target encoding. 6.3 Model [wymaga dopracowania] There is not many packages dedicated to uplift modeling in python. We investigated the two: pylift (“Pylift Package - Documentation,” n.d.) and pyuplift. The latter enables usage of 4 types of models - one of those is Two Model approach. In pylift package there is the TransformedOutcome class that generate predictions. However, the model itself is not well described and uses XGBRegressor unserneath that is not intuitive. Fortunately the package offer also the class UpliftEval that allow uplift metric visualization. In the scene, we decided to create our own classifier (as in the One-Model approach) and use UpliftEval class from pylift model for metric evaluation. As the classifier we used fine-tuned XGBoost. In the figure below we show the cumulative gain chart for train and test sets. FIGURE 6.3: Cumulative gain chart: (left) train set, (right) test set [DODAĆ ZDANIE JAK CZYTAĆ TEN WYKRES - JAK ON POWSTAJE] It can be seen that our model is better than random choice but much worse than practical/theoretical maximum possible. It is also worse than the case without sleeping dogs. It is worth emphesising that our model didn’t experience overfitting as its quality on train and test sets are similar. [napisać gdzieś że miara accuracy jest tu niewłaściwa i poprzeć to dowodem liczbowym] 6.4 Explanations [wymaga dopracowania] The model is already created and the metric show that it brings additional value.Here comes the question whether the model is reliable, does it make the decision based on the features that are important form expert knowledge perspective. Such judgement can be done based on results of XAI tools. We decided to investigate model interpretability from the perspective of the 4 customer groups @(fig:4groups). Therefore we chose one representative customer from each group and analyse the model on instance-level. 6.4.1 Individual perspective In order to explain model output for particular customer we employed Shapley values (???). We benefit from additive feature attribution property of shapley values to model the uplift: UPLIFT=P(PUCHASE|TREATMENT=1) - P(PURCHASE|TREATMENT=0)) –&gt; SHAP(P(PUCHASE|TREATMENT=1)) - SHAP(P(PUCHASE|TREATMENT=0)) = SHAP (UPLIFT) This gives us great opportunity to evaluate these two vectors of Shapley values independently. For example if we use any tree-based model, we can make use of tree-based kernel for shapley value estimation (faster and better convergent) instead of modelling it directly as a black blox model returning difference between two regressors. Experimental results proved, that these two ways of calulcation are providing close estimations, with precision to numerical errors. Below we present Shapley values for the customer with the highest and the lowest uplift computed directly on uplift (without using its additivity) @(fig:upliftSHAP). FIGURE 6.4: Shapley values: (left) customer with the lowest uplift, (right) customer with the highest uplift [DODAĆ WNIOSKI] In a table @(tab:upliftTABLE) there is a comparison of Shapley values obtained using two methods for the customer with the lowest uplift. Column.name Uplift Diff recency -0.0059 -0.0057 history -0.2735 -0.2750 mens -0.0095 -0.0091 womens -0.0568 -0.0550 zip_code_Suburban -0.0005 -0.0022 zip_code_Rural -0.0325 -0.0322 zip_code_Urban -0.0011 -0.0009 newbie 0.0046 0.0040 channel_Phone 0.0007 0.0009 channel_Web 0.0014 0.0012 chennel_Multichannel -0.0006 -0.0007 segment 0.0005 0.0002 Experimental results proved, that these two ways of calulcation are providing close estimations, with precision to numerical errors [TRZEBA NAPISAĆ TEŻ O WPŁYWIE LOSOWOŚCI - LOSUJEMY SUBSET] Conclusions In case of our model there is no need to apply LIME as its main advantages - sparsity - is not important as we have few columns. 6.4.2 Data scientist perspective Unfortunately, its impossible to calculate directly Permutation Feature Importance, because of the previously mentioned problem with lack of full information in both cases: Will the client make the purchse after treatment, and will he without it. Because of having in disposal only historical data (not an oracle), we have only one of these two informations. However, we can make use of the previously computed shapley values of uplift to calculate the same value of permutational feature importance as an average of local shapley importance (defined in a permutational way itself, however calculated in a smarter manner, more in (Lundberg and Lee 2017). We decided to evaluate feature importance not from the well-known dataset-level but subset-level. We extracted from the dataset 3 groups: “sleeping dogs”, “persuadables” and “no impact” (this group is a merge of the groups: “sure things” and “lost causes”). The division was based on the predicted uplift. Sleeping dogs have negative uplift, “no impact” have uplift from zero to the defined epsilon and persuadables have uplift greater than epsilon. We decided to not take into consideration epsilon in case of sleeping dogs as we want to be more conservative. The worst thing the company can do is to discourage the customer from buying. FIGURE 6.5: Variable importance - “sleeping dogs” FIGURE 6.6: Variable importance - “no impact” FIGURE 6.7: Variable importance - “persuadables” Conclusions [W TEJ CZĘŚCI DODAMY JESZCZE PDP] 6.5 Summary and conclusions Using XAI for uplift modeling helps to understand its complex models better. The analysis goes beyond just assesing whether the model is reliable… it can help the executive to understand better the company customers - their behaviour without paying for some extra surveys to investigate their attitude towards the company. A vital part of our work was adjusting XAI techniques for the particularities of uplift modeling. We found out that thanks its additivity Shapley values are well suited for uplift modelling - we showed two methods of using it. We identified limitations of well-known Permutation Feature Importance in terms of explaining uplift modeling. It is caused by the fact that unlike in other supervised models here we do not have exactly labels. Therefore we used the generalization of Shapley values that converge to Permutation Feature Importance. We employed the analysis for the three gropus of customers based on the corresponding uplift. ………………………………….. Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? References "],
["examples.html", "Examples", " Examples "],
["story-covid19.html", "Chapter 7 Story covid19: eXplainable predictions for mortality 7.1 Figures and citations 7.2 Introduction 7.3 Model 7.4 Explanations 7.5 Summary and conclusions", " Chapter 7 Story covid19: eXplainable predictions for mortality Authors: Author 1 (University 1), Author 2 (University 2), Author 3 (University 3) Mentors: Mentor 1 (Affiliation 1), Mentor 2 (Affiliation 2) 7.1 Figures and citations Cite other materials this way: This book is created with (Xie 2015). Refer to figures or chatpers this way: In Figure 7.1 we show a CC BY-NA-SA logo. The next chapter is 8. FIGURE 7.1: Licence: Creative Commons Attribution NonCommercial ShareAlike 7.2 Introduction Put a description of the problem here. indicate the data source. Describe why this problem is important. Indicate the most important literature on the problem. 7.3 Model Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. 7.4 Explanations Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 7.5 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? References "],
["story-lungs.html", "Chapter 8 Story lungs: eXplainable predictions for post operational risks 8.1 Introduction 8.2 Model 8.3 Explanations 8.4 Summary and conclusions", " Chapter 8 Story lungs: eXplainable predictions for post operational risks Authors: Author 1 (University 1), Author 2 (University 2), Author 3 (University 3) Mentors: Mentor 1 (Affiliation 1), Mentor 2 (Affiliation 2) Put a description of the problem here. indicate the data source. Describe why this problem is important. Indicate the most important literature on the problem. 8.1 Introduction 8.2 Model Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. 8.3 Explanations Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 8.4 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? "],
["story-compas.html", "Chapter 9 Story COMPAS: recidivism reloaded 9.1 Introduction 9.2 Model 9.3 Explanations 9.4 Summary and conclusions", " Chapter 9 Story COMPAS: recidivism reloaded Authors: Author 1 (University 1), Author 2 (University 2), Author 3 (University 3) Mentors: Mentor 1 (Affiliation 1), Mentor 2 (Affiliation 2) 9.1 Introduction Put a description of the problem here. indicate the data source. Describe why this problem is important. Indicate the most important literature on the problem. 9.2 Model Place a description of the model(s) here. Focus on key information on the design and quality of the model(s) developed. 9.3 Explanations Here, show how XAI techniques can be used to solve the problem. Will dataset specific or instance specific techniques help more? Will XAI be useful before (pre), during (in) or after (post) modeling? What is interesting to learn from the XAI analysis? 9.4 Summary and conclusions Here add the most important conclusions related to the XAI analysis. What did you learn? Where were the biggest difficulties? What else did you recommend? "],
["acknowledgements.html", "Acknowledgements", " Acknowledgements This project is inspired by a fantastic book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich. We used the LIML project as cornerstone for this reopsitory. "],
["references.html", "References", " References Akshay Kumar, Rishabh Kumar. 2018. “Uplift Modeling : Predicting Incremental Gains.” 2018. http://cs229.stanford.edu/proj2018/report/296.pdf. Hillstrom, Kevin. 2008. “The Minethatdata E-Mail Analytics and Data Mining Challenge Dataset.” 2008. https://blog.minethatdata.com/2008/03/minethatdata-e-mail-analytics-and-data.html. Lee, Josh Xin Jie. 2018. “Simple Machine Learning Techniques to Improve Your Marketing Strategy: Demystifying Uplift Models.” 2018. https://medium.com/datadriveninvestor/simple-machine-learning-techniques-to-improve-your-marketing-strategy-demystifying-uplift-models-dc4fb3f927a2. Lundberg, Scott, and Su-In Lee. 2017. “A Unified Approach to Interpreting Model Predictions.” In. “Pylift Package - Documentation.” n.d. https://pylift.readthedocs.io/en/latest/. R Core Team. 2018. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rzepakowski, Piotr, and Szymon Jaroszewicz. 2012. “Decision Trees for Uplift Modeling with Single and Multiple Treatments.” Knowledge and Information Systems - KAIS 32 (August). https://doi.org/10.1007/s10115-011-0434-0. scikit-learn. 2019a. GradientBoostingRegressor Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html. ———. 2019b. GridSearchCV Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html. ———. 2019c. OneHotEncoder Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html. ———. 2019d. StandardScaler Documentation. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html. Verbeke, W., and C. Bravo. 2017. Profit Driven Business Analytics: A Practitioner’s Guide to Transforming Big Data into Added Value. Wiley and Sas Business Series. Wiley. https://books.google.pl/books?id=NCA3DwAAQBAJ. Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.name/knitr/. Zaniewicz, Lukasz, and Szymon Jaroszewicz. 2013. “Support Vector Machines for Uplift Modeling.” In 2013 Ieee 13th International Conference on Data Mining Workshops, 131–38. IEEE. "]
]
